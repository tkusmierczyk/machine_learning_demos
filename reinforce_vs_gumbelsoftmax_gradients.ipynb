{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convergence and gradients of the Reinforce vs Gumbel-Softmax algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a simple optimization problem with the following loss:\n",
    "$$\\mathcal{L} = \\mathbb{E}_{q(y)} \\left( \\sum_k (y_k - t_k)^2 \\right) $$\n",
    "where $t$ is a $K$-dimensional target variable and $y$ are categorical (discrete; one-hot encoded) samples. We search for $q$ s.t. the objective is minimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.5.0', '0.12.2')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__, tfp.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretization via straight-through:\n",
    "\n",
    "def st(y):\n",
    "    K = y.shape[-1]\n",
    "    y_hard = tf.cast(tf.round(y), y.dtype) if K==1 else tf.cast(tf.one_hot(tf.argmax(y,-1), K), y.dtype)  \n",
    "    y = tf.stop_gradient(y_hard - y) + y  \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can run the experiment for a 1D Categorical variable or for a Bernoulli variable:\n",
    "\n",
    "## Categorical\n",
    "target = tf.constant([0.2, 0.7, 0.1])\n",
    "RDist = tfp.distributions.OneHotCategorical # reference distribution\n",
    "Dist = tfp.distributions.RelaxedOneHotCategorical # relaxed distribution\n",
    "\n",
    "## Bernoulli\n",
    "# target = tf.constant([0.9])\n",
    "# RDist = tfp.distributions.Bernoulli  # reference distribution\n",
    "# Dist = tfp.distributions.RelaxedBernoulli  # relaxed distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Reinforce algorithm converges to zero loss and allocates all the probability mass at the most likely value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.Variable([1.0]*len(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. loss=-0.9532 grad=[ 0.10523976 -0.24215983  0.13692033] distribution=[0.334 0.336 0.33 ]\n",
      "500. loss=-0.6878 grad=[ 0.09391441 -0.21017459  0.11626166] distribution=[0.1321 0.7617 0.1062]\n",
      "1000. loss=-0.2996 grad=[ 0.0473575  -0.07348796  0.02613076] distribution=[0.0517 0.9022 0.0461]\n",
      "1500. loss=-0.3034 grad=[ 0.03656086 -0.07109018  0.03452918] distribution=[0.0332 0.9394 0.0274]\n",
      "2000. loss=-0.1985 grad=[ 0.01989121 -0.04195623  0.022065  ] distribution=[0.0235 0.9597 0.0168]\n",
      "2500. loss=-0.1576 grad=[ 0.01646709 -0.03144655  0.01497903] distribution=[0.0167 0.9698 0.0135]\n",
      "3000. loss=-0.1076 grad=[ 0.00807082 -0.01964078  0.01157041] distribution=[0.0147 0.9733 0.012 ]\n",
      "3500. loss=-0.1447 grad=[ 0.00607743 -0.02726551  0.02118774] distribution=[0.0124 0.9761 0.0115]\n",
      "4000. loss=-0.1250 grad=[ 0.01095055 -0.02302097  0.01207027] distribution=[0.0096 0.9823 0.0081]\n",
      "4500. loss=-0.1082 grad=[ 0.01117846 -0.01941768  0.00823947] distribution=[0.0088 0.9842 0.007 ]\n",
      "5000. loss=-0.1118 grad=[ 0.01018634 -0.01989031  0.00970391] distribution=[0.0072 0.9864 0.0064]\n",
      "5500. loss=-0.1050 grad=[ 0.00575658 -0.01825059  0.01249399] distribution=[0.0065 0.988  0.0055]\n",
      "6000. loss=-0.0612 grad=[ 0.00248522 -0.00974346  0.00725827] distribution=[0.0061 0.9889 0.005 ]\n",
      "6500. loss=-0.0798 grad=[ 0.00596443 -0.01327272  0.00730826] distribution=[0.0053 0.99   0.0047]\n",
      "7000. loss=-0.0736 grad=[ 0.00603805 -0.01206784  0.00603004] distribution=[0.0054 0.9913 0.0033]\n",
      "7500. loss=-0.0342 grad=[ 0.0027081  -0.00479356  0.00208548] distribution=[0.005  0.9914 0.0036]\n",
      "8000. loss=-0.0284 grad=[ 0.00162065 -0.00375013  0.00212949] distribution=[0.0035 0.9924 0.0041]\n",
      "8500. loss=-0.0646 grad=[ 0.00961694 -0.01041997  0.00080294] distribution=[0.0041 0.9907 0.0052]\n",
      "9000. loss=-0.0594 grad=[ 0.00167912 -0.00921653  0.00753764] distribution=[0.0032 0.9934 0.0034]\n",
      "9500. loss=-0.0368 grad=[ 0.00172538 -0.0052816   0.00355622] distribution=[0.0038 0.9932 0.003 ]\n"
     ]
    }
   ],
   "source": [
    "eta = 0.01\n",
    "for i in range(10000):    \n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        d = RDist(logits=logits)\n",
    "        y = d.sample(1000)\n",
    "        loss1 = tf.reduce_sum( (tf.cast(y, target.dtype)-target)**2, -1) * d.log_prob(y) \n",
    "        loss = tf.reduce_mean(loss1)\n",
    "    \n",
    "    grad = tape.gradient(loss, logits)    \n",
    "    if i%500==0:\n",
    "        print(f\"{i}. loss={loss:.4f} grad={grad} \"\n",
    "          f\"distribution={tf.reduce_mean( tf.cast(RDist(logits=logits).sample(10000), tf.float32), 0)}\")\n",
    "    \n",
    "    logits.assign(logits-eta*grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gumbel softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gumbel-Softmax gradient is biased and therefore, the algorithm never achieves zero loss. The bias can be removed by annealing the temperature hyperparmeter down to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.Variable([1.0]*len(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. loss=0.8886 grad=[ 0.02230619 -0.05010794  0.02780175] distribution=[0.3359 0.3287 0.3354]\n",
      "500. loss=0.7864 grad=[ 0.01358167 -0.03801492  0.02443325] distribution=[0.2998 0.405  0.2952]\n",
      "1000. loss=0.7070 grad=[ 0.01298843 -0.02954884  0.0165604 ] distribution=[0.2814 0.4716 0.247 ]\n",
      "1500. loss=0.6660 grad=[ 0.0073073  -0.02408819  0.01678089] distribution=[0.2549 0.5233 0.2218]\n",
      "2000. loss=0.6188 grad=[ 0.00434065 -0.01833881  0.01399817] distribution=[0.2374 0.5643 0.1983]\n",
      "2500. loss=0.6006 grad=[ 0.00465481 -0.01618235  0.01152754] distribution=[0.2339 0.586  0.1801]\n",
      "3000. loss=0.5496 grad=[ 0.00165005 -0.01009198  0.00844193] distribution=[0.2189 0.6071 0.174 ]\n",
      "3500. loss=0.5524 grad=[ 0.00320495 -0.01043981  0.00723486] distribution=[0.2022 0.6426 0.1552]\n",
      "4000. loss=0.5092 grad=[-0.00021451 -0.0050528   0.00526731] distribution=[0.2052 0.6499 0.1449]\n",
      "4500. loss=0.5070 grad=[-0.00038981 -0.004733    0.00512281] distribution=[0.1933 0.6634 0.1433]\n",
      "5000. loss=0.4958 grad=[-0.00180894 -0.00329369  0.00510262] distribution=[0.192 0.664 0.144]\n",
      "5500. loss=0.5052 grad=[-0.00154034 -0.00462544  0.00616578] distribution=[0.2036 0.6582 0.1382]\n",
      "6000. loss=0.5068 grad=[ 0.00091226 -0.00525978  0.00434751] distribution=[0.1901 0.6763 0.1336]\n",
      "6500. loss=0.5046 grad=[-0.00024446 -0.00461741  0.00486188] distribution=[0.1935 0.6704 0.1361]\n",
      "7000. loss=0.4786 grad=[-0.00258102 -0.00108951  0.00367054] distribution=[0.1901 0.6827 0.1272]\n",
      "7500. loss=0.4714 grad=[ 0.00049767 -0.00088576  0.00038809] distribution=[0.1885 0.6866 0.1249]\n",
      "8000. loss=0.4520 grad=[-0.00307628  0.00199783  0.00107846] distribution=[0.1845 0.6947 0.1208]\n",
      "8500. loss=0.4596 grad=[-0.0031116   0.00107958  0.00203203] distribution=[0.1898 0.6873 0.1229]\n",
      "9000. loss=0.4348 grad=[-4.3523167e-03  4.3216464e-03  3.0666473e-05] distribution=[0.1951 0.6846 0.1203]\n",
      "9500. loss=0.4682 grad=[-0.00426835  0.00060489  0.00366346] distribution=[0.1948 0.6898 0.1154]\n"
     ]
    }
   ],
   "source": [
    "eta = 0.01\n",
    "for i in range(10000):    \n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        p = Dist(logits=logits, temperature=5.0)\n",
    "        y = st(p.sample(1000))\n",
    "        loss1 = tf.reduce_sum( (y-target)**2, -1)    \n",
    "        loss = tf.reduce_mean(loss1)\n",
    "\n",
    "    grad = tape.gradient(loss, logits)    \n",
    "    if i%500==0:\n",
    "        print(f\"{i}. loss={loss:.4f} grad={grad} \"\n",
    "          f\"distribution={tf.reduce_mean( st(Dist(logits=logits, temperature=0.001).sample(10000)), 0)}\")\n",
    "    \n",
    "    logits.assign(logits-eta*grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixture of Discrete Normalizing Flows (MDNF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MDNF is biased in a similar way e.g. the bias can be controlled by the temperature hyperparameter. The approximation uses hovewere a different parametrization and therefore the optimization and gradients behave different from the Gumbel-Softmax relaxation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Let's start by downloading and importing the necessary library:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/tkusmierczyk/mixture_of_discrete_normalizing_flows.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"mixture_of_discrete_normalizing_flows/mdnf/\")\n",
    "\n",
    "from flows_factorized_mixture import FactorizedDiscreteFlowsMixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*MDNF uses a different parametrization and therefore we create logits in a different way:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.01\n",
    "p = FactorizedDiscreteFlowsMixture(N=1, K=len(target), B=10, temperature=5.)\n",
    "logits = p.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt = tf.keras.optimizers.Adam(learning_rate=eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. loss=0.8000 distribution=[0.3 0.4 0.3]\n",
      "500. loss=0.8200 distribution=[0.2 0.4 0.4]\n",
      "1000. loss=0.8200 distribution=[0.2 0.4 0.4]\n",
      "1500. loss=0.7200 distribution=[0.1 0.5 0.4]\n",
      "2000. loss=0.7200 distribution=[0.1 0.5 0.4]\n",
      "2500. loss=0.7200 distribution=[0.1 0.5 0.4]\n",
      "3000. loss=0.6000 distribution=[0.1 0.6 0.3]\n",
      "3500. loss=0.6000 distribution=[0.1 0.6 0.3]\n",
      "4000. loss=0.5800 distribution=[0.2 0.6 0.2]\n",
      "4500. loss=0.3600 distribution=[0.1 0.8 0.1]\n",
      "5000. loss=0.2600 distribution=[0.  0.9 0.1]\n",
      "5500. loss=0.3800 distribution=[0.  0.8 0.2]\n",
      "6000. loss=0.3600 distribution=[0.1 0.8 0.1]\n",
      "6500. loss=0.3600 distribution=[0.1 0.8 0.1]\n",
      "7000. loss=0.2400 distribution=[0.1 0.9 0. ]\n",
      "7500. loss=0.2600 distribution=[0.  0.9 0.1]\n",
      "8000. loss=0.2600 distribution=[0.  0.9 0.1]\n",
      "8500. loss=0.1400 distribution=[0. 1. 0.]\n",
      "9000. loss=0.1400 distribution=[0. 1. 0.]\n",
      "9500. loss=0.1400 distribution=[0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10000):    \n",
    "    #p.temperature = 1000./(i+1)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = st(p.sample(100)[:,0,:])\n",
    "        loss1 = tf.reduce_sum( (y-target)**2, -1)    \n",
    "        loss = tf.reduce_mean(loss1)\n",
    "\n",
    "    grad = tape.gradient(loss, logits)    \n",
    "    if i%500==0:\n",
    "        y = st(p.sample(10000)[:,0,:])\n",
    "        print(f\"{i}. loss={loss:.4f} \"\n",
    "              f\"distribution={tf.reduce_mean(y, 0)}\"\n",
    "             )\n",
    "    \n",
    "    #opt.apply_gradients(zip([grad], [logits]))\n",
    "    logits.assign(logits-eta*grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
