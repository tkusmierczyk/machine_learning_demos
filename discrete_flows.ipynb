{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete Flows "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I explain very basics of and demonstrate how to use discrete flows introduced in:\n",
    "\n",
    "*Tran, Dustin, et al. \"Discrete flows: Invertible generative models of discrete data.\" Advances in Neural Information Processing Systems. 2019.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure that all the necessary things are installed\n",
    "# !pip install tensorflow tensorflow_probability\n",
    "# !pip install \"git+https://github.com/google/edward2.git#egg=edward2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import edward2 as ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.1.0', '0.9.0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tested with versions\n",
    "tf.__version__, tfp.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations on One-hot encoded vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look on one-hot encoded arithmetics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([0., 0., 0., 1., 0.]) # x=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = tf.constant([0., 0., 1., 0., 0.]) # shift = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I round output tesnor (x_transformed) since one_hot_add implementation in Edward2 is numerically noisy. In practice it's actually better to use one_hot_minus!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_transformed = ed.layers.utils.one_hot_add(x, mu) # x_transformed = x + mu\n",
    "np.round(x_transformed, 4) # (4+2) mod 5 == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's learn mu s.t. x_target = mu + x (learning here is trivial we just train one variable) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different initalizations!\n",
    "np.random.seed(9661)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where do we start\n",
    "x = tf.constant([1., 0., 0., 0. , 0., 0.], name=\"x\") # =0\n",
    "\n",
    "# what we want to get\n",
    "x_target = tf.constant([0., 0., 0., 0., 0., 1.], name=\"x_target\") # =5\n",
    "\n",
    "# trainable variable in unconstrained space\n",
    "mu_logits = tf.Variable(np.random.randn(6), name=\"mu_logits\") # random initialization\n",
    "#mu_logits = tf.Variable([0.1, 0.1, 0.1, 0.1, 0.1, 0.1], name=\"mu_logits\") # uniform initialization\n",
    "\n",
    "# straigh-through estimator of mu = argmax(mu_logits) \n",
    "# where mu is one-hot representation of a shift transformation\n",
    "#  the temperature controls bias of gradients going through the argmax\n",
    "mu = ed.layers.utils.one_hot_argmax(mu_logits, temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(6,), dtype=float32, numpy=array([1., 0., 0., 0., 0., 0.], dtype=float32)>,\n",
       " <tf.Tensor: shape=(6,), dtype=float32, numpy=array([0., 0., 0., 0., 0., 1.], dtype=float32)>,\n",
       " <tf.Variable 'mu_logits:0' shape=(6,) dtype=float64, numpy=\n",
       " array([-0.42223113,  2.03345075, -1.60760614, -0.51772096,  1.21881057,\n",
       "        -1.65165981])>,\n",
       " <tf.Tensor: shape=(6,), dtype=float64, numpy=array([0., 1., 0., 0., 0., 0.])>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview tensors\n",
    "x, x_target, mu_logits, mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient-based learning of mu_logits so argmax(mu_logits) would learn to move x to x_target. We observe how mu changes so x+mu would ==x_target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=0 mu=[0. 1. 0. 0. 0. 0.] x_transformed=[0. 1. 0. 0. 0. 0.]\n",
      "iter=1 mu=[0. 1. 0. 0. 0. 0.] x_transformed=[0. 1. 0. 0. 0. 0.]\n",
      "iter=2 mu=[0. 0. 0. 0. 1. 0.] x_transformed=[0. 0. 0. 0. 1. 0.]\n",
      "iter=3 mu=[0. 1. 0. 0. 0. 0.] x_transformed=[0. 1. 0. 0. 0. 0.]\n",
      "iter=4 mu=[0. 0. 0. 0. 1. 0.] x_transformed=[0. 0. 0. 0. 1. 0.]\n",
      "iter=5 mu=[0. 1. 0. 0. 0. 0.] x_transformed=[0. 1. 0. 0. 0. 0.]\n",
      "iter=6 mu=[0. 0. 0. 0. 1. 0.] x_transformed=[0. 0. 0. 0. 1. 0.]\n",
      "iter=7 mu=[0. 1. 0. 0. 0. 0.] x_transformed=[0. 1. 0. 0. 0. 0.]\n",
      "iter=8 mu=[0. 0. 0. 0. 1. 0.] x_transformed=[0. 0. 0. 0. 1. 0.]\n",
      "iter=9 mu=[1. 0. 0. 0. 0. 0.] x_transformed=[1. 0. 0. 0. 0. 0.]\n",
      "iter=10 mu=[0. 1. 0. 0. 0. 0.] x_transformed=[0. 1. 0. 0. 0. 0.]\n",
      "iter=11 mu=[0. 0. 0. 1. 0. 0.] x_transformed=[0. 0. 0. 1. 0. 0.]\n",
      "iter=12 mu=[0. 0. 0. 0. 1. 0.] x_transformed=[0. 0. 0. 0. 1. 0.]\n",
      "iter=13 mu=[0. 1. 0. 0. 0. 0.] x_transformed=[0. 1. 0. 0. 0. 0.]\n",
      "iter=14 mu=[0. 0. 0. 0. 0. 1.] x_transformed=[0. 0. 0. 0. 0. 1.]\n",
      "iter=15 mu=[0. 0. 0. 0. 0. 1.] x_transformed=[0. 0. 0. 0. 0. 1.]\n",
      "iter=16 mu=[0. 0. 0. 0. 0. 1.] x_transformed=[0. 0. 0. 0. 0. 1.]\n",
      "iter=17 mu=[0. 0. 0. 0. 0. 1.] x_transformed=[0. 0. 0. 0. 0. 1.]\n",
      "iter=18 mu=[0. 0. 0. 0. 0. 1.] x_transformed=[0. 0. 0. 0. 0. 1.]\n",
      "iter=19 mu=[0. 0. 0. 0. 0. 1.] x_transformed=[0. 0. 0. 0. 0. 1.]\n",
      "iter=20 mu=[0. 0. 0. 0. 0. 1.] x_transformed=[0. 0. 0. 0. 0. 1.]\n",
      "iter=21 mu=[0. 0. 0. 0. 0. 1.] x_transformed=[0. 0. 0. 0. 0. 1.]\n",
      "iter=22 mu=[0. 0. 0. 0. 0. 1.] x_transformed=[0. 0. 0. 0. 0. 1.]\n",
      "iter=23 mu=[0. 0. 0. 0. 0. 1.] x_transformed=[0. 0. 0. 0. 0. 1.]\n",
      "iter=24 mu=[0. 0. 0. 0. 0. 1.] x_transformed=[0. 0. 0. 0. 0. 1.]\n",
      "iter=25 mu=[0. 0. 0. 0. 0. 1.] x_transformed=[0. 0. 0. 0. 0. 1.]\n",
      "iter=26 mu=[0. 0. 0. 0. 0. 1.] x_transformed=[0. 0. 0. 0. 0. 1.]\n",
      "iter=27 mu=[0. 0. 0. 0. 0. 1.] x_transformed=[0. 0. 0. 0. 0. 1.]\n",
      "iter=28 mu=[0. 0. 0. 0. 0. 1.] x_transformed=[0. 0. 0. 0. 0. 1.]\n",
      "iter=29 mu=[0. 0. 0. 0. 0. 1.] x_transformed=[0. 0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.RMSprop(lr=0.1)\n",
    "\n",
    "for i in range(30):\n",
    "    with tf.GradientTape() as tape: \n",
    "        \n",
    "        mu = ed.layers.utils.one_hot_argmax(mu_logits, temperature=1.0)\n",
    "        x_transformed = ed.layers.utils.one_hot_add(x, mu)        \n",
    "        loss = tf.reduce_sum((x_target-x_transformed)**2) # squared loss\n",
    "        \n",
    "        if i%1==0:\n",
    "            #print(np.round(mu, 1).reshape(-1))\n",
    "            print(\"iter=%i mu=%s x_transformed=%s\" % \n",
    "                   (i, np.round(mu, 2), np.round(abs(x_transformed), 1)))        \n",
    "            \n",
    "    gradients = tape.gradient(loss, mu_logits)        \n",
    "    optimizer.apply_gradients([(gradients, mu_logits)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'mu_logits:0' shape=(6,) dtype=float64, numpy=\n",
       "array([1.09168971, 1.04800181, 0.51804335, 1.11984215, 1.14813777,\n",
       "       1.18473996])>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's look on how logits changed\n",
    "mu_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming batches of N-dimensional samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sample from N-dimensional K-categorical distribution a batch of samples and pass it through a flow that is modelling dependecies between dimensions using an autoregressive transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, K = 2, 3 # two variables with three categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(123) # assure the same results every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[0.1, 0.1, 0.8],\n",
       "       [0.2, 0.2, 0.6]], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# base distribution\n",
    "probs = [[0.1, 0.1, 0.8],[0.2,0.2,0.6]]\n",
    "base = tfp.distributions.OneHotCategorical(probs=probs)\n",
    "base.probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\n",
       "array([[[1, 0, 0],\n",
       "        [0, 0, 1]],\n",
       "\n",
       "       [[0, 0, 1],\n",
       "        [0, 0, 1]]], dtype=int32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample batch of two samples from base distribution -> output dim = (batch, N, K)\n",
    "sample = base.sample(2)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use a masked autoncoder to model our transformation mu\n",
    "mu = ed.layers.MADE(K, hidden_dims=[3,3], hidden_order=\"left-to-right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an autorgressive flow using transformation mu\n",
    "flow = ed.layers.DiscreteAutoregressiveFlow(mu, temperature=0.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\n",
       "array([[[1, 0, 0],\n",
       "        [1, 0, 0]],\n",
       "\n",
       "       [[0, 0, 1],\n",
       "        [0, 1, 0]]], dtype=int32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's push forward our sample and see how values changed \n",
    "# (=ones were moved to other positions)\n",
    "transformed_sample = flow(sample)\n",
    "transformed_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.,  0., -0.],\n",
       "        [ 0., -0.,  1.]],\n",
       "\n",
       "       [[ 0., -0.,  1.],\n",
       "        [-0., -0.,  1.]]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and retrieve the original sample by pushing the transformed_sample back\n",
    "restored_sample = flow.reverse(transformed_sample)\n",
    "np.round(restored_sample, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ True,  True,  True],\n",
       "        [ True,  True,  True]],\n",
       "\n",
       "       [[ True,  True,  True],\n",
       "        [ True,  True,  True]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare the original sample with the retrieved one\n",
    "np.round(restored_sample, 4)==np.round(sample, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training larger transformation with MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the flow transformation so base samples passed through the flow would follow a distribution as close as possbile to a target distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[0.7, 0.2, 0.1],\n",
       "       [0.3, 0.4, 0.3]], dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'true' data generating distribution\n",
    "target = tfp.distributions.OneHotCategorical(probs = [[0.7, 0.2, 0.1],[0.3,0.4,0.3]])\n",
    "target.probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100, 2, 3])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# our 'features'\n",
    "target_samples = tf.cast(target.sample(100), 'float32') # cast to the right type\n",
    "target_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=0 loss=3.371\n",
      "iter=1 loss=2.270\n",
      "iter=2 loss=2.873\n",
      "iter=3 loss=2.873\n",
      "iter=4 loss=2.873\n",
      "iter=5 loss=2.928\n",
      "iter=6 loss=2.928\n",
      "iter=7 loss=2.325\n",
      "iter=8 loss=2.325\n",
      "iter=9 loss=2.325\n",
      "iter=10 loss=2.325\n",
      "iter=20 loss=2.325\n",
      "iter=30 loss=2.928\n",
      "iter=40 loss=2.325\n",
      "iter=50 loss=2.873\n",
      "iter=60 loss=2.237\n",
      "iter=70 loss=2.237\n",
      "iter=80 loss=2.237\n",
      "iter=90 loss=2.840\n",
      "iter=100 loss=2.237\n",
      "iter=110 loss=2.237\n",
      "iter=120 loss=2.237\n",
      "iter=130 loss=2.237\n",
      "iter=140 loss=2.237\n",
      "iter=150 loss=2.237\n",
      "iter=160 loss=2.237\n",
      "iter=170 loss=2.237\n",
      "iter=180 loss=2.840\n",
      "iter=190 loss=2.215\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.RMSprop(lr=0.1)\n",
    "\n",
    "for i in range(200):\n",
    "    with tf.GradientTape() as tape: \n",
    "        # move samples to the space where we know how to evaluate probabilities\n",
    "        reversed_target_samples = flow.reverse(target_samples)\n",
    "        \n",
    "        # evaluate log-probs of the samples (output shape=batch x N)\n",
    "        # (i.e., log_probs = base.log_prob(reversed_target_samples) )\n",
    "        probs = tf.reduce_sum(reversed_target_samples*base.probs, -1)\n",
    "        log_probs = tf.math.log(probs+1e-31)\n",
    "        \n",
    "        # independent variables -> we just sum up log-probs \n",
    "        # to get joint log prob of a N-dim sample\n",
    "        log_probs = tf.reduce_sum(log_probs, -1) \n",
    "        \n",
    "        # loss = minus average log-likelihood\n",
    "        loss = -tf.reduce_mean(log_probs) \n",
    "\n",
    "        if i%10==0 or i<10:        \n",
    "            print(\"iter=%i loss=%.3f\" % (i, loss))\n",
    "            \n",
    "    gradients = tape.gradient(loss, flow.trainable_variables)        \n",
    "    optimizer.apply_gradients(zip(gradients, flow.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8 , 0.1 , 0.1 ],\n",
       "       [0.24, 0.56, 0.2 ]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's retrieve a resulting distribution \n",
    "#  'generated' by passing samples from the base through the flow\n",
    "np.round( tf.reduce_mean(flow(tf.cast(base.sample(100000),'float32')),0), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[0.7, 0.2, 0.1],\n",
       "       [0.3, 0.4, 0.3]], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we compare it against the target distribution\n",
    "target.probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[0.1, 0.1, 0.8],\n",
       "       [0.2, 0.2, 0.6]], dtype=float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we note that it's not exactly the same\n",
    "# but let's a look again at the base distribution\n",
    "base.probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the target distribution here was factorized, so actually the network mu did not need to learn any dependecies between dimensions, but in general case target could be any distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
